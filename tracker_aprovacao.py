#!/usr/bin/env python3
"""
Bot de Rastreamento de Concursos ETEC/FATEC
Crawler aut√¥nomo que descobre processos seletivos no portal do CPS (URH),
baixa documentos publicados e verifica se o nome do candidato aparece.

Tamb√©m monitora o Di√°rio Oficial do Estado de SP (DOE SP) buscando
cita√ß√µes do nome do candidato em publica√ß√µes oficiais.

Extrai metadados (edital, unidade, cidade, disciplina) e identifica
a fase do processo (Abertura ‚Üí Classifica√ß√£o ‚Üí Convoca√ß√£o‚Ä¶).

Notifica√ß√µes via WhatsApp (CallMeBot).
"""

import io
import json
import os
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urljoin

import pdfplumber
import requests
from bs4 import BeautifulSoup
from docx import Document as DocxDocument

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONFIGURA√á√ÉO
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MEU_NOME = os.getenv("MEU_NOME", "Renan Bezerra dos Santos")

# CallMeBot ‚Äì WhatsApp
CALLMEBOT_PHONE = os.getenv("CALLMEBOT_PHONE", "")
CALLMEBOT_APIKEY = os.getenv("CALLMEBOT_APIKEY", "")

# Caminho do hist√≥rico de documentos j√° processados
HISTORY_FILE = Path(__file__).parent / "history_pdfs.json"

# Headers para simular navegador comum
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

REQUEST_TIMEOUT = 30  # segundos

# Base do portal CPS
CPS_BASE = "https://urhsistemas.cps.sp.gov.br"

# P√°ginas de listagem de processos (Inscri√ß√µes Abertas + Em Andamento)
LISTING_PAGES: list[dict] = [
    # ‚îÄ‚îÄ ETEC ‚îÄ‚îÄ
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/ETEC/PSS/Abertos.aspx",
     "label": "ETEC ‚Äì Processo Seletivo Docente ‚Äì Inscri√ß√µes Abertas"},
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/ETEC/PSS/Andamento.aspx",
     "label": "ETEC ‚Äì Processo Seletivo Docente ‚Äì Em Andamento"},
    {"url": f"{CPS_BASE}/dgsdad/selecaopublica/ETEC/CPD/Abertos.aspx",
     "label": "ETEC ‚Äì Concurso P√∫blico Docente ‚Äì Inscri√ß√µes Abertas"},
    {"url": f"{CPS_BASE}/dgsdad/selecaopublica/ETEC/CPD/emAndamento.aspx",
     "label": "ETEC ‚Äì Concurso P√∫blico Docente ‚Äì Em Andamento"},
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/ETEC/Auxiliar/EmAndamento.aspx",
     "label": "ETEC ‚Äì Auxiliar de Docente ‚Äì Em Andamento"},
    # ‚îÄ‚îÄ FATEC ‚îÄ‚îÄ
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/FATEC/PSS/inscricoesabertas.aspx",
     "label": "FATEC ‚Äì Processo Seletivo Docente ‚Äì Inscri√ß√µes Abertas"},
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/FATEC/ProcessoSeletivo/EmAndamento.aspx",
     "label": "FATEC ‚Äì Processo Seletivo Docente ‚Äì Em Andamento"},
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/FATEC/CPD/Abertos.aspx",
     "label": "FATEC ‚Äì Concurso P√∫blico Docente ‚Äì Inscri√ß√µes Abertas"},
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/FATEC/CPD/emAndamento.aspx",
     "label": "FATEC ‚Äì Concurso P√∫blico Docente ‚Äì Em Andamento"},
    # ‚îÄ‚îÄ PSSAD (Auxiliar de Docente ‚Äì compartilhado ETEC/FATEC) ‚îÄ‚îÄ
    {"url": f"{CPS_BASE}/dgsdad/SelecaoPublica/PSSAD/Abertos.aspx",
     "label": "PSSAD ‚Äì Auxiliar de Docente ‚Äì Inscri√ß√µes Abertas"},
    {"url": f"{CPS_BASE}/dgsdad/selecaopublica/PSSAD/emAndamento.aspx",
     "label": "PSSAD ‚Äì Auxiliar de Docente ‚Äì Em Andamento"},
]

# Limite de processos por p√°gina de listagem (evita sobrecarga)
MAX_PROCESSES_PER_PAGE = 50

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAPA DE FASES ‚Äì classifica o documento pelo nome/link
# Ordem reflete a progress√£o real do processo seletivo.
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

PHASE_MAP: list[tuple[str, str]] = [
    # (padr√£o regex case-insensitive, r√≥tulo amig√°vel)
    (r"edital\s*de\s*abertura|EDITALDE?ABERTURA", "üìã Edital de Abertura"),
    (r"redu[c√ß][a√£]o.*isen[c√ß][a√£]o|REDUCAO.ISENCAO", "üí∞ Resultado Redu√ß√£o/Isen√ß√£o de Taxa"),
    (r"reabertura|REABERTURA", "üîÑ Reabertura de Inscri√ß√µes"),
    (r"banca\s*examinadora|BANCAEXAMINADORA", "üë• Portaria da Banca Examinadora"),
    (r"altera[c√ß][a√£]o.*cronograma|ALTERACAOCRONOGRAMA", "üìÖ Altera√ß√£o de Cronograma"),
    (r"altera[c√ß][a√£]o.*comiss[a√£]o|ALTERACAOCOMISSAO", "üîÄ Altera√ß√£o da Comiss√£o"),
    (r"deferimento|indeferimento|DEFERIMENTO", "‚úÖ Deferimento/Indeferimento de Inscri√ß√µes"),
    (r"resultado.*escrita.*conv|RESULTADOESCRITACONV|resultado.*pve", "üìù Resultado Prova Escrita e Convoca√ß√£o Did√°tica"),
    (r"resultado.*memorial|resultado.*prova|RESULTADO", "üìù Resultado de Prova/Avalia√ß√£o"),
    (r"classifica[c√ß][a√£]o\s*final|CLASSIFICAOFINAL|CLASSIFICACAOFINAL", "üèÜ Classifica√ß√£o Final"),
    (r"homologa[c√ß][a√£]o|HOMOLOGA", "‚úîÔ∏è Homologa√ß√£o"),
    (r"convoca[c√ß][a√£]o|CONVOCAO|CONVOCACAO", "üìû Convoca√ß√£o"),
    (r"prorroga[c√ß][a√£]o|PRORROGA", "‚è≥ Prorroga√ß√£o de Validade"),
]


def classify_phase(doc_name: str, doc_url: str) -> str:
    """Identifica a fase do processo a partir do nome do documento ou URL."""
    combined = f"{doc_name} {doc_url}"
    for pattern, phase_label in PHASE_MAP:
        if re.search(pattern, combined, re.IGNORECASE):
            return phase_label
    return "üìÑ Documento"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DI√ÅRIO OFICIAL DO ESTADO DE SP (DOE SP)
# API p√∫blica: do-api-web-search.doe.sp.gov.br
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

DOE_API_BASE = "https://do-api-web-search.doe.sp.gov.br"
DOE_SITE_BASE = "https://www.doe.sp.gov.br"

# ID do caderno "Executivo" no DOE SP
DOE_JOURNAL_EXECUTIVO = "ca96256b-6ca1-407f-866e-567ef9430123"

# Quantos dias para tr√°s buscar no DOE (janela de busca)
DOE_SEARCH_DAYS = 30

# M√°ximo de resultados por p√°gina na API do DOE
DOE_PAGE_SIZE = 20


def search_doe_sp(name: str, history: dict) -> tuple[dict, int]:
    """
    Busca o nome do candidato no Di√°rio Oficial do Estado de SP
    via API p√∫blica. Retorna (history atualizado, qtd novos).
    """
    today = datetime.now()
    from_date = (today - timedelta(days=DOE_SEARCH_DAYS)).strftime("%Y-%m-%d")
    to_date = today.strftime("%Y-%m-%d")

    new_count = 0
    page = 1

    while True:
        params = {
            "Terms": name,
            "FromDate": from_date,
            "ToDate": to_date,
            "JournalId": DOE_JOURNAL_EXECUTIVO,
            "PageNumber": page,
            "PageSize": DOE_PAGE_SIZE,
            "SortField": "Date",
        }

        try:
            resp = requests.get(
                f"{DOE_API_BASE}/v2/advanced-search/publications",
                params=params,
                headers=HEADERS,
                timeout=REQUEST_TIMEOUT,
            )
            resp.raise_for_status()
            data = resp.json()
        except (requests.RequestException, ValueError) as e:
            print(f"  [ERRO] Falha na busca DOE SP (p√°gina {page}): {e}")
            break

        items = data.get("items", [])
        if not items:
            break

        for item in items:
            pub_id = item.get("id", "")
            doe_key = f"doe:{pub_id}"

            if doe_key in history:
                continue

            title = item.get("title", "Sem t√≠tulo")
            slug = item.get("slug", "")
            hierarchy = item.get("hierarchy", "")
            excerpt = item.get("excerpt", "")
            pub_date = item.get("date", "")[:10]
            pub_url = f"{DOE_SITE_BASE}/{slug}" if slug else ""
            matches = item.get("totalTermsFound", 0)

            print(f"    [DOE NOVO] {title}")
            print(f"      Hierarquia: {hierarchy}")
            print(f"      Men√ß√µes: {matches}")

            history[doe_key] = {
                "source": "DOE-SP",
                "title": title,
                "date": pub_date,
                "hierarchy": hierarchy,
                "url": pub_url,
                "matches": matches,
                "found_name": True,
            }
            new_count += 1

            # Montar mensagem WhatsApp
            msg = (
                "üì∞ *SEU NOME NO DI√ÅRIO OFICIAL!* üì∞\n\n"
                f"üìå *Publica√ß√£o:* {title}\n"
                f"üìÖ *Data:* {pub_date}\n"
                f"üèõÔ∏è *Se√ß√£o:* {hierarchy}\n"
                f"üîé *Men√ß√µes encontradas:* {matches}\n"
            )
            if excerpt:
                # Limitar excerpt para n√£o estourar mensagem
                short_excerpt = excerpt[:300]
                if len(excerpt) > 300:
                    short_excerpt += "‚Ä¶"
                msg += f"üìù *Trecho:* _{short_excerpt}_\n"
            if pub_url:
                msg += f"üîó *Link:* {pub_url}"

            send_whatsapp(msg)

        # Pr√≥xima p√°gina
        if not data.get("hasNextPage", False):
            break
        page += 1

    return history, new_count


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HIST√ìRICO
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def load_history() -> dict:
    """Carrega o JSON com os documentos j√° processados."""
    if HISTORY_FILE.exists():
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_history(history: dict) -> None:
    """Salva o JSON atualizado."""
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# NOTIFICA√á√ÉO ‚Äì WHATSAPP (CallMeBot)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def send_whatsapp(message: str) -> None:
    """Envia mensagem via CallMeBot WhatsApp API."""
    if not CALLMEBOT_PHONE or not CALLMEBOT_APIKEY:
        print("[AVISO] CallMeBot n√£o configurado. Mensagem apenas no log:")
        print(message)
        return

    url = "https://api.callmebot.com/whatsapp.php"
    params = {
        "phone": CALLMEBOT_PHONE,
        "text": message,
        "apikey": CALLMEBOT_APIKEY,
    }
    try:
        resp = requests.get(url, params=params, timeout=REQUEST_TIMEOUT)
        if resp.status_code == 200:
            print("[OK] Mensagem WhatsApp enviada.")
        else:
            print(f"[ERRO] CallMeBot retornou status {resp.status_code}: {resp.text}")
    except requests.RequestException as e:
        print(f"[ERRO] Falha ao enviar WhatsApp: {e}")

    # Respeitar rate-limit do CallMeBot (m√≠n. 2 s entre mensagens)
    time.sleep(3)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CRAWLER ‚Äì DESCOBERTA DE PROCESSOS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _get_soup(url: str) -> BeautifulSoup | None:
    """Faz GET e retorna BeautifulSoup ou None em caso de erro."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "html.parser")
    except requests.RequestException as e:
        print(f"  [ERRO] N√£o foi poss√≠vel acessar {url}: {e}")
        return None


def discover_detail_links(listing_url: str) -> list[str]:
    """
    Acessa uma p√°gina de listagem (GridView) e extrai os links
    das p√°ginas de detalhes dos processos seletivos.
    Retorna at√© MAX_PROCESSES_PER_PAGE URLs √∫nicas.
    """
    soup = _get_soup(listing_url)
    if soup is None:
        return []

    detail_links: list[str] = []
    seen: set[str] = set()

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        # P√°ginas de detalhes cont√™m o par√¢metro oljioahohafnav87412
        if "oljioahohafnav87412" not in href:
            continue
        # Ignorar links javascript:__doPostBack (s√£o os cabe√ßalhos de ordena√ß√£o)
        if href.startswith("javascript:"):
            continue
        full_url = urljoin(listing_url, href)
        if full_url not in seen:
            seen.add(full_url)
            detail_links.append(full_url)

    return detail_links[:MAX_PROCESSES_PER_PAGE]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# EXTRA√á√ÉO DE METADADOS DA P√ÅGINA DE DETALHES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_metadata(soup: BeautifulSoup) -> dict:
    """
    Extrai metadados do processo a partir do texto da p√°gina de detalhes.
    Retorna dict com: edital, unidade, cidade, disciplina/curso.
    """
    text = soup.get_text(" ", strip=True)

    meta: dict = {
        "edital": "",
        "unidade": "",
        "cidade": "",
        "disciplina": "",
    }

    # N¬∫ do Edital ‚Äì ex: "EDITAL DE ABERTURA N¬∫  229/11/2026"
    m = re.search(
        r"EDITAL\s+DE\s+ABERTURA\s+N[¬∫o¬∞]\s*([\d/]+)",
        text, re.IGNORECASE,
    )
    if m:
        meta["edital"] = m.group(1).strip()

    # Unidade de Ensino e Cidade
    # Padr√£o: "C√ìD. DA UNIDADE:  229 - UNIDADE DE ENSINO:  Escola ... - CIDADE: S√£o Paulo"
    m = re.search(
        r"UNIDADE\s+DE\s+ENSINO:\s*(.+?)\s*-\s*CIDADE:\s*(.+?)(?:\n|CURSO|DISCIPLINA|COMPONENTE|REQUISITO|Os pedidos|Per[i√≠]odo)",
        text, re.IGNORECASE,
    )
    if m:
        meta["unidade"] = m.group(1).strip()
        meta["cidade"] = m.group(2).strip()

    # Disciplina ou Componente Curricular
    m = re.search(
        r"(?:DISCIPLINA|COMPONENTE\s+CURRICULAR):\s*(?:\d+\s*-\s*)?(.+?)(?:\n|REQUISITO|Os pedidos|Per[i√≠]odo)",
        text, re.IGNORECASE,
    )
    if m:
        meta["disciplina"] = m.group(1).strip()

    # Se n√£o achou disciplina, tenta CURSO
    if not meta["disciplina"]:
        m = re.search(
            r"CURSO:\s*(.+?)(?:\n|DISCIPLINA|COMPONENTE|REQUISITO|Os pedidos|Per[i√≠]odo)",
            text, re.IGNORECASE,
        )
        if m:
            meta["disciplina"] = m.group(1).strip()

    return meta


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SCRAPING ‚Äì DOCUMENTOS NA P√ÅGINA DE DETALHES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def fetch_detail_page(detail_url: str) -> tuple[dict, list[dict]]:
    """
    Acessa a p√°gina de detalhes de um processo e retorna:
    1. Metadados (edital, unidade, cidade, disciplina)
    2. Lista de documentos encontrados (PDF e DOCX)
    """
    soup = _get_soup(detail_url)
    if soup is None:
        return {}, []

    meta = extract_metadata(soup)

    docs: list[dict] = []
    doc_pattern = re.compile(r"\.(pdf|docx?)(\?.*)?$", re.IGNORECASE)

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        match = doc_pattern.search(href)
        if not match:
            continue
        full_url = urljoin(detail_url, href)
        name = a_tag.get_text(strip=True) or href.split("/")[-1].split("?")[0]
        ext = match.group(1).lower()
        if ext == "doc":
            ext = "docx"
        phase = classify_phase(name, full_url)
        docs.append({"name": name, "url": full_url, "ext": ext, "phase": phase})

    return meta, docs


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# AN√ÅLISE DE DOCUMENTOS (PDF e DOCX)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _download(url: str) -> bytes | None:
    """Baixa um arquivo na mem√≥ria e retorna os bytes."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT * 2)
        resp.raise_for_status()
        return resp.content
    except requests.RequestException as e:
        print(f"  [ERRO] Falha ao baixar {url}: {e}")
        return None


def check_name_in_pdf(content: bytes, name: str) -> bool:
    """Verifica se o nome aparece em um PDF (case insensitive)."""
    name_lower = name.lower()
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                if name_lower in text.lower():
                    return True
    except Exception as e:
        print(f"  [ERRO] Falha ao ler PDF: {e}")
    return False


def check_name_in_docx(content: bytes, name: str) -> bool:
    """Verifica se o nome aparece em um DOCX (case insensitive)."""
    name_lower = name.lower()
    try:
        doc = DocxDocument(io.BytesIO(content))
        for para in doc.paragraphs:
            if name_lower in para.text.lower():
                return True
        # Verificar tamb√©m tabelas
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if name_lower in cell.text.lower():
                        return True
    except Exception as e:
        print(f"  [ERRO] Falha ao ler DOCX: {e}")
    return False


def check_name_in_document(url: str, ext: str, name: str) -> bool:
    """Baixa o documento e verifica se o nome aparece."""
    content = _download(url)
    if content is None:
        return False
    if ext == "pdf":
        return check_name_in_pdf(content, name)
    elif ext in ("docx", "doc"):
        return check_name_in_docx(content, name)
    return False


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# FORMATA√á√ÉO DE MENSAGENS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _format_meta_block(meta: dict, label: str) -> str:
    """Monta o bloco de informa√ß√µes do processo para a mensagem."""
    lines: list[str] = []
    if meta.get("edital"):
        lines.append(f"üìå *Edital:* {meta['edital']}")
    if meta.get("unidade"):
        lines.append(f"üè´ *Unidade:* {meta['unidade']}")
    if meta.get("cidade"):
        lines.append(f"üìç *Cidade:* {meta['cidade']}")
    if meta.get("disciplina"):
        lines.append(f"üìö *Disciplina:* {meta['disciplina']}")
    lines.append(f"üóÇÔ∏è *Tipo:* {label}")
    return "\n".join(lines)


def build_message_found(doc_name: str, doc_url: str, phase: str,
                        meta: dict, label: str, detail_url: str) -> str:
    """Mensagem quando o nome √â encontrado no documento."""
    return (
        "üö®üö®üö® *SEU NOME FOI ENCONTRADO!* üö®üö®üö®\n\n"
        f"{_format_meta_block(meta, label)}\n\n"
        f"üìÑ *Documento:* {doc_name}\n"
        f"üîñ *Fase:* {phase}\n"
        f"üîó *Arquivo:* {doc_url}\n"
        f"üìã *P√°gina:* {detail_url}"
    )


def build_message_not_found(doc_name: str, doc_url: str, phase: str,
                            meta: dict, label: str, detail_url: str) -> str:
    """Mensagem quando o nome N√ÉO √© encontrado (nova publica√ß√£o)."""
    return (
        "‚ö†Ô∏è *Nova publica√ß√£o detectada*\n\n"
        f"{_format_meta_block(meta, label)}\n\n"
        f"üìÑ *Documento:* {doc_name}\n"
        f"üîñ *Fase:* {phase}\n"
        "Seu nome *n√£o* foi encontrado na busca autom√°tica.\n"
        f"üîó *Arquivo:* {doc_url}\n"
        f"üìã *P√°gina:* {detail_url}"
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# L√ìGICA PRINCIPAL
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def process_detail_page(detail_url: str, label: str, history: dict) -> dict:
    """
    Processa uma p√°gina de detalhes de um processo seletivo.
    Retorna o history atualizado.
    """
    meta, docs = fetch_detail_page(detail_url)
    if not docs:
        return history

    edital_info = meta.get("edital", "?")
    unidade_info = meta.get("unidade", "?")

    for doc_info in docs:
        doc_url = doc_info["url"]
        doc_name = doc_info["name"]
        doc_ext = doc_info["ext"]
        doc_phase = doc_info["phase"]

        if doc_url in history:
            continue

        print(f"    [NOVO] {doc_phase} | {doc_name} (.{doc_ext})")
        found = check_name_in_document(doc_url, doc_ext, MEU_NOME)

        # Registrar no hist√≥rico
        history[doc_url] = {
            "name": doc_name,
            "phase": doc_phase,
            "detail_page": detail_url,
            "listing": label,
            "edital": meta.get("edital", ""),
            "unidade": meta.get("unidade", ""),
            "cidade": meta.get("cidade", ""),
            "disciplina": meta.get("disciplina", ""),
            "found_name": found,
        }

        if found:
            msg = build_message_found(
                doc_name, doc_url, doc_phase, meta, label, detail_url)
            print(f"    >>> NOME ENCONTRADO! Edital {edital_info} ‚Äì {unidade_info} <<<")
        else:
            msg = build_message_not_found(
                doc_name, doc_url, doc_phase, meta, label, detail_url)
            print(f"    Nome n√£o encontrado. Edital {edital_info} ‚Äì {unidade_info}")

        send_whatsapp(msg)

    return history


def main() -> None:
    print("Bot de Rastreamento de Concursos ETEC/FATEC (Crawler Aut√¥nomo)")
    print(f"Nome monitorado: {MEU_NOME}")
    print(f"P√°ginas de listagem: {len(LISTING_PAGES)}")

    history = load_history()
    total_new = 0

    for listing in LISTING_PAGES:
        listing_url = listing["url"]
        label = listing["label"]

        print(f"\n{'='*60}")
        print(f"[LISTAGEM] {label}")
        print(f"  {listing_url}")
        print(f"{'='*60}")

        detail_links = discover_detail_links(listing_url)
        if not detail_links:
            print("  Nenhum processo encontrado nesta p√°gina.")
            continue

        print(f"  {len(detail_links)} processo(s) encontrado(s).")

        for i, detail_url in enumerate(detail_links, 1):
            print(f"  [{i}/{len(detail_links)}] {detail_url}")
            old_count = len(history)
            history = process_detail_page(detail_url, label, history)
            total_new += len(history) - old_count

    # ‚îÄ‚îÄ FASE 2: Di√°rio Oficial do Estado de SP ‚îÄ‚îÄ
    print(f"\n{'='*60}")
    print(f"[DOE SP] Buscando nome no Di√°rio Oficial do Estado de SP")
    print(f"  Per√≠odo: √∫ltimos {DOE_SEARCH_DAYS} dias")
    print(f"{'='*60}")

    history, doe_new = search_doe_sp(MEU_NOME, history)
    total_new += doe_new

    if doe_new == 0:
        print("  Nenhuma publica√ß√£o nova encontrada no DOE SP.")
    else:
        print(f"  {doe_new} publica√ß√£o(√µes) nova(s) no DOE SP.")

    save_history(history)
    print(f"\n{'='*60}")
    print(f"Execu√ß√£o finalizada.")
    print(f"  Documentos novos processados: {total_new}")
    print(f"  Total no hist√≥rico: {len(history)}")
    print(f"  Hist√≥rico salvo em {HISTORY_FILE}")


if __name__ == "__main__":
    main()
